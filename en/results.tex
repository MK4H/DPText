\chapter{Results}
\label{sec:results}

In this chapter, we first describe our setup for measurement and validation of the implementations described in the previous chapter. We then compare the definition-based cross-correlation implementations against each other before comparing them with an FFT-based CUDA implementation. Finally, we compare the definition-based implementations with existing real-world cross-correlation implementations in the Python SciPy library and in Matlab.

\section{Experiments}
\label{sec:experiments}
As the main aim of this thesis is to compare implementations of an algorithm, the code is heavily instrumented to enable measurements and comparisons of different parts of the implementation. This instrumentation is designed to limit its impact and to allow measurements that minimize background noise and imprecision of the time measurement tools provided by the CUDA C++ language.

As part of this thesis, we have also developed a benchmarking tool that allows the use of a declarative description of the set of benchmarks to be executed. The tool generates input and validation data, runs the benchmarks for all inputs and with all argument combinations specified, and records the times and validation results. As such, the tool is used both for measurements and for validation of the implementation. 

To simplify the implementation of the definition-based algorithm optimizations, we have placed several restrictions on the input matrices and the computation:
\begin{itemize}
	\item both input matrices are of the same size,
	\item whole output matrix is computed.
	% TODO: If I do not manage to finish it in time, this should be fixed and expanded
\end{itemize}

Both restrictions are used to simplify the implementation and reduce the number of variables when measuring. Both restrictions could be removed in production-grade implementation but would make the optimizations unnecessarily harder to implement. 

\subsection{Code instrumentation}
\label{sec:code_instrumentation}

All implementations, including definition-based, FFT-based, and CPU-based, are split into the following steps:

\begin{itemize}
	\item \textit{Load} loads the input matrices into host memory,
	\item \textit{Prepare} allocates device memory and precomputes things derived from the input data size,
	\item \textit{Transfer} moves data from host to device memory,
	\item \textit{Run} executes the computation,
	\item \textit{Finalize} moves data from device to host memory,
	\item \textit{Free} releases resources allocated in the \textit{Prepare} step,
	\item \textit{Store} stores results from host memory.
\end{itemize}

Each of these steps can be individually measured and compared. The main focus of this thesis is the \textit{Run} step, but to properly compare the behavior of the FFT-based implementations with definition-based implementations, we will have to compare other steps as well.

We also provide simple CPU-based single-threaded definition-based implementation, for which most of these steps are empty. This implementation is provided for the basic validation of results and of the benchmarking infrastructure. 


The code instrumentation allows us to measure the algorithms with three different levels of granularity:
\begin{itemize}
	\item \textit{Compute} measuring the duration of the Prepare, Transfer, Run, Finalize, and Free steps together;
	\item \textit{CommonSteps} measuring every step separately;
	\item \textit{Algorithm} measuring algorithm-specific parts such as individual kernels or library calls.
\end{itemize}

For parts that can be executed repeatedly, such as computations and data transfers, we utilize measurement with an adaptive iteration count. The number of iterations is automatically increased until the measured duration is longer than a configured minimum, most often a second. This type of measurement should mitigate background noise and get around problems with minimum clock resolution for very short steps. It is used for the \textit{Compute} measurement, for measuring the \textit{Run} step, and for measuring kernel and function call durations in each algorithm.

For timing the SciPy implementation, we use the \texttt{perf\_counter\_ns} function provided by the Python standard library. For Matlab, the pair of functions named \texttt{tic} and \texttt{toc} is used to measure the execution time. For both implementations, we also utilize the adaptive iteration count. For both, we measure with only the \textit{Compute} granularity.


\subsection{Experiment setup}

Experiments were run on two systems:
\begin{itemize}
	\item Intel Xeon Silver 4110 with 256GB of RAM and NVIDIA Tesla V100 PCIe 16 GB (gpulab), running Rocky Linux 8.5, gcc 11.2, CUDA 11.6, and nvidia driver 510.47.03;
	\item AMD Ryzen 5 4600H with 16GB of RAM and NVIDIA GeForce RTX 2060 (laptop), running Ubuntu 20.04, gcc 9.4.0, CUDA 11.4, and nvidia driver 470.129.06.
\end{itemize}

The two systems allow us to evaluate our implementations on two different classes of GPU hardware, gpulab representing the enterprise server systems with Tesla GPU and Xeon CPU, and laptop representing consumer hardware with gaming GeForce card and Ryzen CPU. The two systems also allow us to compare two different GPU generations, Compute Capability 7.0 for the V100 and Compute capability 7.5 for the RTX 2060. Unless stated otherwise, the results showcased in this text are measured on gpulab. Laptop results will be added only when the behavior significantly differs from that of gpulab, as the algorithms are in no way optimized for any of the two cards and should run similarly in both cases. 

Most showcased results are collected using the adaptive iteration count. The timing is done using CUDA events when measuring the run time of CUDA kernels or the high-resolution clock provided by the C++ standard library for all other measurements. The resulting value shown is the total measured time divided by the number of iterations. To further remove noise, all measurements are repeated 20 times, and the mean of these measurements is taken as the final result, as there are no significant outliers that would skew the mean.



\subsection{Result validation}


When validating the results of a computation, we compare them to valid results computed using SciPy, Matlab, or our simple CPU definition-based implementation. The output matrix is compared element by element with the valid result, computing a matrix of differences using formula \ref{eq:result_comparison} \citep{wiki:relative_difference}.

% See https://en.wikipedia.org/wiki/Relative_change_and_difference
% and https://c-faq.com/fp/fpequal.html
\begin{equation}
\label{eq:result_comparison}
\mathrm{Relative\_difference }(a, b) = \frac{\abs{a - b}}{\max(\abs{a}, \abs{b})}
\end{equation}

The maximum element in the matrix of differences is then taken as the error of the output matrix.

\section{Comparing definition-based algorithms}
\label{sec:results_definition_based}

In this section, we first measure and compare the members of the Warp shuffle algorithm family, described in Section \ref{sec:warp_shuffle_alg}, against each other. Next, we compare the members of the Warp per shift algorithm family, introduced in Section \ref{sec:warp_per_shift}. Lastly, we compare the best optimizations from both algorithm families with the Basic definition-based implementation.


Each of the \textit{one-to-one}, \textit{one-to-many}, \textit{n-to-mn}, and \textit{n-to-m} input types is compared separately, as it allows for different optimizations and may benefit certain implementations better than others. In the following sections, we list and use only the best-performing arguments for each implementation. The benchmarks measuring behavior with different argument values which were used to choose these optimal arguments are available in the thesis attachments in the directory \texttt{code/benchmarking/args\_test}. The results were left out of the text of the thesis for brevity, as they closely follow the descriptions of each optimization.  

When comparing the definition-based algorithms, we measure only the kernels in the \textit{Run} step, as the implementation of this step is the only difference between the algorithms. As such, the speedup reported in this section represents only the change in execution time of the \textit{Run} step, i.e., the computation itself, not including allocations, loading from disk, transfers to GPU, etc.

Implementations utilizing warp shuffle instructions are usable across a range of input sizes. We measure their behavior for input matrix sizes ranging from 16x16 to 512x512 elements. These input matrix sizes were chosen as the larger input sizes are faster computed using the FFT-based implementation, and as such the speed of the optimizations of the definition-based implementation is irrelevant for the larger sizes. Based on the input type, we also measure with a different number of left and right input matrices to gauge the changes in the behavior of the implementations with a changing number of input matrices.


\subsection{Warp shuffle optimizations with the one-to-one input}
\label{sec:results_warp_shuffle_one_to_one}
For this input type, we have the following four implementations with these arguments:

\begin{center}
	\begin{tabular}{|l|l|c|} 
		\hline
		Implementation&Argument&Value\\ [0.5ex] 
		\hline\hline
		Simple & Warps per thread block & 4 \\
		\hline
 		\multirow{3}*{Simple with work distribution} & Warps per thread block & 8\\
 		\cline{2-3}
 		& Rows per thread & 1 \\
 		\cline{2-3}
 		& Distribution type & triangle \\
 		\hline
 		\multirow{2}*{Multirow right} & Warps per thread block & 4\\
 		\cline{2-3}
 		& Right rows per thread & 8\\
 		\hline
 		\multirow{3}*{Multirow both} & Warps per thread block & 4\\
 		\cline{2-3}
 		& Shifts per thread & 8\\
 		\cline{2-3}
 		& Left rows per iteration & 4\\
		\hline
	\end{tabular}
\end{center}

\begin{figure}[ht]
	\centering	
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		% Must be relative to current directory
		% as input ignores graphicspath, which is
		% only for includegraphics{}
		\input{./img/warp_shuffle_one_to_one_gpulab.pdf_tex}
		\caption{gpulab}
		\label{fig:warp_shuffle_one_to_one_results_gpulab}
	\end{subfigure}
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		% Must be relative to current directory
		% as input ignores graphicspath, which is
		% only for includegraphics{}
		\input{./img/warp_shuffle_one_to_one.pdf_tex}
		\caption{laptop}
		\label{fig:warp_shuffle_one_to_one_results_laptop}
	\end{subfigure}

	
	\caption{Speedup of \textit{one-to-one} warp shuffle optimizations.}
	\label{fig:warp_shuffle_one_to_one_results}
\end{figure}

The results displayed in Figure \ref{fig:warp_shuffle_one_to_one_results} show the speedup of each optimization compared to the Simple warp shuffle implementation. For smaller inputs on gpulab, the \textit{multirow} optimizations are slower, up to 31\% slower for the \textit{multirow\_both} and up to 68\% slower for the \textit{multirow\_right} than the Simple implementation. This slowdown is caused by the reduction in the number of threads and correspondingly reduced occupancy of the GPU. Due to the smaller size of the laptop GPU, the reduced number of threads has a much lower effect. For \textit{multirow\_right}, this is combined with each thread reading each row of the right input matrix multiple times, adding global memory access latency, which the GPU is unable to hide due to the low occupancy. This problem is mitigated in the \textit{multirow\_both} optimization, which is reflected here in better performance for all input sizes when compared with the \textit{multirow\_right} optimization. For larger input sizes, the better ratio of warp shuffle to fused multiply-add instructions of these two optimizations allows them to overtake the Simple implementation. This is above 128x128 for \textit{multirow\_both} and above 320x320 for \textit{multirow\_right} on gpulab. For laptop, the lower performance of the GPU increases the effect of the optimizations, overtaking the Simple implementation sooner. 

The behavior of work distribution optimization tells us that the gpulab GPU is not fully saturated by the Simple implementation up until the 320x320 input matrix size. Again due to the smaller size of the laptop GPU, it is saturated much sooner. Low occupancy is an expected problem with the \textit{one-to-one} input type. On gpulab for inputs smaller than 320x320, the large number of additional threads allows us to fully utilize the GPU, making the work distribution optimization more than four times faster than the Simple implementation without work distribution for certain input matrix sizes. As we increase the input size, the benefit of additional threads diminishes, completely disappearing above 320x320 input size, where the overhead introduced by the additional threads negates the increased GPU saturation. 

The maximum absolute time improvement is for the 512x512 input size, improving from $67.18$~ms to $24.08$~ms on gpulab and from $303$~ms to $72$~ms on laptop.
% Make just one diagram showing the four implementations and their behavior across the input sizes
% Mention the optimal arguments used


\subsection{Warp shuffle optimizations with the one-to-many input}
This input type has six implementations with the following arguments:

\begin{center}
	\begin{tabular}{|l|l|c|} 
		\hline
		Implementation&Argument&Value\\ [0.5ex] 
		\hline\hline
		Simple & Warps per thread block & 4 \\
		\hline
		\multirow{2}*{Multimat right} & Warps per thread block & 4 \\
 		\cline{2-3}
 		& Right matrices per thread & 8 \\
		\hline
		\multirow{4}*{Multimat right with work distribution} & Warps per thread block & 8 \\
		\cline{2-3}
		& Right matrices per thread & 8 \\
		\cline{2-3}
		& Rows per thread & 1 \\
		\cline{2-3}
		& Distribution type & triangle \\
		\hline
		\multirow{3}*{Multirow right multimat right} & Warps per thread block & 4 \\
		\cline{2-3}
		& Right rows per thread & 2 \\
		\cline{2-3}
		& Right matrices per thread & 4 \\
		\hline
		\multirow{4}*{Multirow both multimat right} & Warps per thread block & 4 \\
		\cline{2-3}
		& Shifts per right matrix & 4 \\
		\cline{2-3}
		& Right matrices per thread & 4 \\
		\cline{2-3}
		& Left rows per iteration & 4 \\
		\hline
	\end{tabular}
\end{center}

\begin{figure}[ht]
	\centering	
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/warp_shuffle_one_to_many_2_matrices_gpulab.pdf_tex}
		%\label{fig:executed_instructions_shared_mem}
	\end{subfigure}
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/warp_shuffle_one_to_many_8_matrices_gpulab.pdf_tex}
		%\label{fig:pipeline_utilization_shared_mem}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/warp_shuffle_one_to_many_16_matrices_gpulab.pdf_tex}
		%\label{fig:executed_instructions_shared_mem}
	\end{subfigure}
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/warp_shuffle_one_to_many_1024_matrices_gpulab.pdf_tex}
		%\label{fig:pipeline_utilization_shared_mem}
	\end{subfigure}
	
	\caption{Speedup of \textit{one-to-many} warp shuffle optimizations on gpulab.}
	\label{fig:warp_shuffle_one_to_many_results}
\end{figure}

\begin{figure}[ht]
	\centering	
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/warp_shuffle_one_to_many_8_matrices_gpulab.pdf_tex}
		\caption{gpulab}
		\label{fig:warp_shuffle_one_to_many_results_gpulab}
	\end{subfigure}
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/warp_shuffle_one_to_many_8_matrices.pdf_tex}
		\caption{laptop}
		\label{fig:warp_shuffle_one_to_many_results_laptop}
	\end{subfigure}
	
	\caption{Comparison of the input with 8 right matrices on gpulab and laptop.}
	\label{fig:warp_shuffle_one_to_many_results_gpulab_vs_laptop}
\end{figure}

From the results in Figure \ref{fig:warp_shuffle_one_to_many_results}, we see that most optimizations are more than 50\% slower than the Simple implementation for small input sizes and small numbers of matrices. This is caused by lower occupancy of the GPU, as both \textit{multimat} and \textit{multirow} group multiple overlaps into each job, reducing the total number of jobs and decreasing the total number of workers. The exception is \textit{Multimat right with work distribution}, where the \textit{work distribution} optimization is specifically designed to solve the problem of low occupancy by splitting each overlap into several jobs. This results in the 4 times speedup we see for the input 32x32 with two right input matrices. However, with increasing total input size, be it the size of each matrix or the total number of matrices, the need to increase occupancy diminishes, and the speedup provided by \textit{work distribution} is lower.

For larger input sizes, the improved ratio of warp shuffle instructions to fused multiply-add instructions balances out the decreased occupancy. The combination of \textit{multimat right} and \textit{multirow right} is slightly better than the \textit{multimat right} optimization alone but is hampered by the increased number of reads from global memory as each row from the right input matrix is reread two times by each worker. This is also the reason why the argument \textit{right rows per thread} is best when set to 2, as increasing the value of this argument increases the number of times each row is read by the given worker. This is the reason why for larger inputs, the \textit{multimat right} and its combination with \textit{multirow right} provide similar speedup, as any improvement of the instruction ratio is balanced by the increased number of reads.

The best improvement for the input sizes we are interested in is provided by the combination of \textit{multimat right} with \textit{multirow both}. This combination removes the disadvantage of multiple reads of the \textit{multirow right}, leaving just the improved instruction ratio.

When we compare behavior on gpulab and laptop in Figure \ref{fig:warp_shuffle_one_to_many_results_gpulab_vs_laptop}, we see that the change is very similar to the one shown in Section \ref{sec:results_warp_shuffle_one_to_one}. The optimizations improving occupancy are not as effective, while the optimizations reducing the number of warp shuffle instructions and global memory accesses significantly improve the performance. The measurement in Figure \ref{fig:warp_shuffle_one_to_many_results_laptop}, computing input with 8 right matrices, are very similar to the results from Figure \ref{fig:warp_shuffle_one_to_many_results} with 16 input matrices, i.e., with twice the input data size. This shows the effects of the lower performance and smaller size of the GPU.

The maximum absolute improvement is for the $512 \times 512$ matrix size with 1024 right matrices, where we see a decrease from $67$~s to $16.8$~s on gpulab and from $308$~s to $67$~s on laptop.

\subsection{Warp shuffle optimizations with the n-to-mn input}
This input type shares implementations with the \textit{one-to-many} type, as it does not provide any additional possibilities for data reuse because each left input matrix is cross-correlated with a different set of $m$ right input matrices. This means that each implementation of \textit{n-to-mn} type just executes the \textit{one-to-many} implementation kernel $n$ times in parallel, once for each left input matrix. Due to this, the main factor here is how many of the \textit{one-to-many} kernels can we run in parallel, or more precisely, how many thread blocks from these kernels can fit on a single SM.

The arguments differ slightly due to the higher GPU utilization:

\begin{center}
	\begin{tabular}{|l|l|c|} 
		\hline
		Implementation&Argument&Value\\ [0.5ex] 
		\hline\hline
		Simple & Warps per thread block & 4 \\
		\hline
		\multirow{3}*{Simple with work distribution} & Warps per thread block & 4 \\
		\cline{2-3}
		& Rows per thread & 1 \\
		\cline{2-3}
		& Distribution type & triangle \\
		\hline
		\multirow{2}*{Multimat right} & Warps per thread block & 4 \\
		\cline{2-3}
		& Right matrices per thread & 8 \\
		\hline
		\multirow{4}*{Multimat right with work distribution} & Warps per thread block & 4 \\
		\cline{2-3}
		& Right matrices per thread & 8 \\
		\cline{2-3}
		& Rows per thread & 1 \\
		\cline{2-3}
		& Distribution type & triangle \\
		\hline
		\multirow{4}*{Multirow right multimat right} & Warps per thread block & 4 \\
		\cline{2-3}
		& Right rows per thread & 2 \\
		\cline{2-3}
		& Right matrices per thread & 4 \\
		\cline{2-3}
		& Number of CUDA streams & 16 \\
		\hline
		\multirow{5}*{Multirow both multimat right} & Warps per thread block & 4 \\
		\cline{2-3}
		& Shifts per right matrix & 4 \\
		\cline{2-3}
		& Right matrices per thread & 4 \\
		\cline{2-3}
		& Left rows per iteration & 4 \\
		\cline{2-3}
		& Number of CUDA streams & 16 \\
		\hline
	\end{tabular}
\end{center}

\begin{figure}[ht]
	\centering	
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/warp_shuffle_n_to_mn_2_4_gpulab.pdf_tex}
		%\label{fig:executed_instructions_shared_mem}
	\end{subfigure}
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/warp_shuffle_n_to_mn_4_16_gpulab.pdf_tex}
		%\label{fig:pipeline_utilization_shared_mem}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/warp_shuffle_n_to_mn_100_1000_gpulab.pdf_tex}
		%\label{fig:executed_instructions_shared_mem}
	\end{subfigure}
	
	\caption{Speedup of \textit{n-to-mn} warp shuffle optimizations on gpulab.}
	\label{fig:warp_shuffle_n_to_mn_results}
\end{figure}

These measurements are very similar to the measurements for the \textit{one-to-many} type. The differences in measurements between the same implementation used in these two input types reflect the total resources required by the kernel, i.e., how many kernels can run in parallel.


The results in Figure \ref{fig:warp_shuffle_n_to_mn_results} show that for a small number of small input matrices, the \textit{work distribution} optimization is still necessary to balance out the occupancy reduced by other optimizations. The arguments of the implementations using \textit{work distribution} are optimized for the smallest input sizes. To run with the same arguments for input matrices of size 512x512 would require the CUDA grid size of 65536 in the \textit{y} axis, which is just above the 65535 maximum limit. The same implementations did run for the 512x512 input size in \textit{one-to-many} as they were run with eight warps per thread block, which reduces the required grid size, and for \textit{one-to-many} was faster than four warps per thread block. In the measurements for \textit{n-to-mn} type, the version with four warps per thread block came out faster. 
As work distribution is only faster for the smaller input sizes, we choose to use the four warps per thread even if it cannot run the whole range of input sizes, as we try to optimize the speedup for each size separately.

The explanation of the performance of the \textit{multimat right}, the \textit{multirow right multimat right} and the \textit{multirow both multimat right} optimizations is the same as for the \textit{one-to-many} type, as is the effect of running the benchmarks on gpulab when compared to laptop. The occupancy improving optimizations are less effective, while the efficiency improving optimizations are more effective. 
The maximum absolute improvement is for the $512 \times 512$ matrix size with 100 left matrices to 1000 right matrices, where we see decrease from $65.43$~s to $17.29$~s on gpulab and from $303$~s to $69.49$~s on laptop.


\subsection{Warp shuffle optimizations with the n-to-m input}
The final input type has a group of implementations specifically optimized for this type, the \textit{multimat\_both} optimization and its combinations with other optimizations. We also reuse some \textit{one-to-many} implementations, running them $n$ times in parallel, once for each left input matrix, this time all with the same set of right input matrices.

Again, Figure \ref{fig:warp_shuffle_n_to_m_results} shows that \textit{work distribution} is advantageous for smaller inputs, but is not required for larger inputs. As expected, the \textit{multimat\_both} optimization specifically designed for this input type gives the best performance. For smaller inputs, it is best when combined with \textit{work distribution}. For medium and large inputs, it is best when combined with \textit{multirow\_both} optimization. As \textit{multimat right} improvement does not reuse data from the left input matrices, it falls behind the \textit{multimat\_both}.

The maximum absolute improvement is for the $512 \times 512$ matrix size with 50 left matrices to 50 right matrices, where we see a decrease from $163.95$~s to $31.66$~s on gpulab. For laptop, the input with $512 \times 512$ and 50 left matrices and 50 right matrices does not fit into GPU memory. As such, the maximum absolute improvement is for the size $256 \times 256$ with 50 and 50 matrices, decreasing from $49$~s to $14$~s.

\begin{figure}[ht]
	\centering	
	\begin{subfigure}{0.35\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/warp_shuffle_n_to_m_2_4_gpulab.pdf_tex}
		%\label{fig:executed_instructions_shared_mem}
	\end{subfigure}
	\begin{subfigure}{0.35\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/warp_shuffle_n_to_m_4_16_gpulab.pdf_tex}
		%\label{fig:pipeline_utilization_shared_mem}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.35\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/warp_shuffle_n_to_m_50_50_gpulab.pdf_tex}
		%\label{fig:executed_instructions_shared_mem}
	\end{subfigure}
	
	\caption{Speedup of \textit{n-to-m} warp shuffle optimizations on gpulab.}
	\label{fig:warp_shuffle_n_to_m_results}
\end{figure}

\begin{center}
	\begin{tabular}{|l|l|c|} 
		\hline
		Implementation&Argument&Value\\ [0.5ex] 
		\hline\hline
		\multirow{2}*{Simple} & Warps per thread block & 4 \\
		\cline{2-3}
		& Number of CUDA streams & 8 \\
		\hline
		\multirow{3}*{Multimat right} & Warps per thread block & 4 \\
		\cline{2-3}
		& Right matrices per thread & 8 \\
		\cline{2-3}
		& Number of CUDA streams & 8 \\
		\hline
		\multirow{5}*{Multimat right with work distribution} & Warps per thread block & 4 \\
		\cline{2-3}
		& Right matrices per thread & 8 \\
		\cline{2-3}
		& Rows per thread & 1 \\
		\cline{2-3}
		& Distribution type & triangle \\
		\cline{2-3}
		& Number of CUDA streams & 8 \\
		\hline
		\multirow{3}*{Multimat both} & Warps per thread block & 4 \\
		\cline{2-3}
		& Left matrices per thread & 4 \\
		\cline{2-3}
		& Right matrices per thread & 4 \\
		\hline
		\multirow{5}*{Multimat both with work distribution} & Warps per thread block & 4 \\
		\cline{2-3}
		& Left matrices per thread & 4 \\
		\cline{2-3}
		& Right matrices per thread & 4 \\		
		\cline{2-3}
		& Rows per thread & 1 \\
		\cline{2-3}
		& Distribution type & triangle \\
		\hline
		\multirow{5}*{Multirow both multimat both} & Warps per thread block & 4 \\
		\cline{2-3}
		& Left matrices per thread & 4 \\
		\cline{2-3}
		& Right matrices per thread & 4 \\		
		\cline{2-3}
		& Shifts per right matrix & 4 \\
		\cline{2-3}
		& Left rows per iteration & 4 \\
		\hline
	\end{tabular}
\end{center}





\subsection{Warp per shift optimizations with the one-to-one inputs}
\label{sec:results_occupancy_improvements}

Occupancy is mainly a concern when working with the \textit{one-to-one} input type and small input matrices. Because of this, we implement the algorithms from this family mostly for the \textit{one-to-one} input type and will compare them only for this input type. 

The following table lists the Warp per shift algorithm family implementations, together with the Block per shift algorithm implementation, with the arguments used for their measurement:

\begin{center}
	\begin{tabular}{|l|l|c|} 
		\hline
		Implementation&Argument&Value\\ [0.5ex] 
		\hline\hline
		Warp per shift & Shifts per thread block & 16 \\
		\hline
		\multirow{3}*{Warp per shift with work distribution} & Shifts per thread block & 8\\
		\cline{2-3}
		& Rows per warp & 10 \\
		\cline{2-3}
		& Distribution type & triangle \\
		\hline
		\multirow{4}*{Warp per shift with shared memory} & Shifts per thread block & 16\\
		\cline{2-3}
		& Shared memory row size & 128\\
		\cline{2-3}
		& Load with stride & True\\
		\cline{2-3}
		& Column group per block & True\\
		\hline
		Block per shift & Block size & 256\\
		\hline
	\end{tabular}
\end{center}

From the results in Figure \ref{fig:warp_per_shift_results}, we see that the base Warp per shift implementation is enough to saturate the GPU. Due to this, the Block per shift algorithm and Work distribution optimization do not improve the run time. While still slower than the base Warp per shift implementation, the increasing speed of work distribution with increasing input size is most likely caused by the increasing size of each job and correspondingly decreasing proportion of the overhead caused by distributing the jobs and collecting the results.

\begin{figure}[ht]
	\centering	
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		% Must be relative to current directory
		% as input ignores graphicspath, which is
		% only for includegraphics{}
		\input{./img/warp_per_shift_one_to_one_gpulab.pdf_tex}
		\caption{gpulab}
		\label{fig:warp_per_shift_results_gpulab}
	\end{subfigure}
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		% Must be relative to current directory
		% as input ignores graphicspath, which is
		% only for includegraphics{}
		\input{./img/warp_per_shift_one_to_one.pdf_tex}
		\caption{laptop}
		\label{fig:warp_per_shift_results_laptop}
	\end{subfigure}
	\caption{Relative speed of warp per shift optimizations.}
	\label{fig:warp_per_shift_results}
\end{figure}

The shared memory optimization is not beneficial for inputs smaller than 64x64, mostly due to adding synchronization overhead between warps of a thread block when loading data into shared memory. For larger input data sizes, the reduced number of global memory accesses gives this optimization an advantage over the base Warp per shift implementation.

The largest absolute improvement in run time is for the $512 \times 512$ input matrix size from $0.158$~s to $0.074$~s.



\subsection{Comparison with Basic algorithm}
\label{sec:comparison_with_basic}

Now that we have compared the optimizations of both families separately, we now compare the best of these optimizations against the Basic definition-based implementation described in Section \ref{sec:basic_alg}. As the implementations still differ only in the GPU kernel, we again compare only the time taken by the Run step.


\begin{figure}[ht]
	\centering	
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/definition_based_speedup_one_to_one_gpulab.pdf_tex}
		\caption{One to one.}
		\label{fig:definition_based_speedup_one_to_one}
	\end{subfigure}
	%\hfill
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/definition_based_speedup_one_to_many_gpulab.pdf_tex}
		\caption{One to many.}
		\label{fig:definition_based_speedup_one_to_many}
	\end{subfigure}
	%\hfill
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/definition_based_speedup_n_to_mn_gpulab.pdf_tex}
		\caption{N to MN.}
		\label{fig:definition_based_speedup_n_to_mn}
	\end{subfigure}
	%\hfill
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/definition_based_speedup_n_to_m_gpulab.pdf_tex}
		\caption{N to M.}
		\label{fig:definition_based_speedup_n_to_m}
	\end{subfigure}
	\caption{Speedup of the best optimized implementation compared to Basic implementation on gpulab.}
	\label{fig:definition_based_speedup}
\end{figure}

Figure \ref{fig:definition_based_speedup} shows the achieved speedup for each of the four input types by the best optimization when compared to the Basic implementation. We see inputs for which our implementations are up to 80 times faster. For large inputs, the speedup is around five times, i.e., the computation requires one-fifth of the time required by the Basic implementation.

For the \textit{one-to-one} input type shown in Figure \ref{fig:definition_based_speedup_one_to_one}, we separate the Warp shuffle and Warp per shift based optimizations. As Warp per shift optimizations are designed to improve occupancy, they are generally only relevant for the \textit{one-to-one} input type, as the total size of input data is not enough to saturate the GPU using Basic implementation and simple Warp shuffle implementation. But as we see in the results, Warp shuffle combined with the \textit{work distribution} optimization is almost as fast as Warp per shift. For inputs larger than 200x200, we see that the occupancy is not a limiting factor and the data reuse provided by Warp shuffle optimizations becomes more relevant. The maximum absolute time improvement is from $0.072$s to $0.024$s. For all following input types, we use only Warp shuffle implementations as they are always better than Warp per shift implementations.

The \textit{one-to-many} input type shown in Figure \ref{fig:definition_based_speedup_one_to_many} of up to 80 times when computing one left matrix against 1024 right matrices of size 32x32, achieving the highest speedup. For smaller matrix sizes, increasing the number of right matrices improves speed, which indicates that the GPU is not fully utilized. For larger matrix sizes, the difference between speedups for the different number of matrices becomes negligible, with all converging around a speedup of 5. This seems to be the speedup of our best implementations when GPU is fully saturated. The maximum absolute time improvement is from $86.21$s to $16.83$s. 

The \textit{n-to-mn} input type shown in Figure \ref{fig:definition_based_speedup_n_to_mn} shows different behavior to the \textit{one-to-many} type. The speedup spike for smaller matrix sizes caused by \textit{work distribution} is also present, but now with the increasing number of matrices we get a smaller speedup. Above matrix size of 200x200, optimizations with improved data reuse become prominent, causing the improving speedup for larger matrix sizes. The maximum absolute time improvement is from $83.42$s to $17.29$s. 

The \textit{n-to-m} input type shown in Figure \ref{fig:definition_based_speedup_n_to_m} shows similar characteristics to the \textit{n-to-mn} type. We again see the \textit{work distribution} spike for small matrix sizes, transitioning to optimizations providing better data reuse. The maximum absolute time improvement is from $203.28$s to $31.66$s.

\begin{figure}[ht]
	\centering	
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/definition_based_speedup_one_to_one_gpulab.pdf_tex}
		\caption{One to one.}
	\end{subfigure}
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/definition_based_speedup_n_to_mn_gpulab.pdf_tex}
		\caption{N to MN.}
	\end{subfigure}
	\caption{Speedup of the best optimized implementation compared to Basic implementation on laptop.}
	\label{fig:definition_based_speedup_laptop}
\end{figure}

Two two input types that show a significant difference between gpulab and laptop are shown in Figure \ref{fig:definition_based_speedup_laptop}. The \textit{one-to-one} type again shows the quick falloff of occupancy improving optimizations, which become slower than ones improving the instruction ratio or memory accesses with input matrix size 64x64. We see a similar effect on the \textit{n-to-mn} type, where the initial spike for small inputs caused by occupancy improving optimizations is completely gone. The maximum absolute time improvement is from $351$s to $69$s.

\section{Comparison with existing implementations}

In this section, we compare the best of our definition-based implementations as measured in Section \ref{sec:results_definition_based}, with existing 2D cross-correlation implementations. First, we make a comparison against the FFT-based CUDA implementation. Next, we take two existing tools/libraries in the form of Python SciPy \citet{journal:scipy} library and Matlab \citep{site:matlab} which implement 2D cross-correlation, and compare them with our definition-based algorithms.

We have chosen the Python SciPy library as a generally used CPU implementation of 2D cross-correlation, and Matlab with Parallel Computing Toolbox for GPU accelerated 2D cross-correlation. Due to licensing limitations, Matlab will only be compared on the RTX 2060 system.

The methods for time measurement of all three implementations are described in Section \ref{sec:code_instrumentation}. For each input size (combination of matrix size and the number of matrices), we choose the fastest implementation provided by this thesis. Generally, for smaller inputs, the implementations with work distribution optimization will be chosen, whereas for larger inputs, the implementations optimizing for efficiency through reduced global memory accesses, improved instruction ratios, etc., will be chosen. The comparison of the definition-based implementations and which of them are the fastest for a given input is presented in Section \ref{sec:results_definition_based}. 

\subsection{FFT-based implementation}
\label{sec:results_fft_based}

The FFT-based implementation used in this thesis is adapted from the one used by \citet{misko}. It uses the cuFFT library for the Fast Fourier Transform and a custom kernel for Hadamard multiplication. 

Before comparing this FFT-based implementation with the definition-based implementations, we first show a few properties of this implementation caused by the use of the Fast Fourier transform in general and of the cuFFT library in particular. First is the dependency between the precise size of the input matrix and computation time. As described in the cuFFT library documentation, cuFFT provides "Algorithms highly optimized for input sizes that can be written in the form $2^{a}*3^{b}*5^{c}*7^{d}$. In general, the smaller the prime factor, the better the performance, i.e., powers of two are fastest." \citep{site:cufft}. From our measurements, we see up to 30\% 
slowdown between the power of two input sizes and input sizes one smaller. 

\begin{figure}[ht]
	\centering
	\def\svgwidth{0.3\textwidth}
	% Must be relative to current directory
	% as input ignores graphicspath, which is
	% only for includegraphics{}
	\input{./img/fft_small_compute_time.pdf_tex}
	\caption{Comparison of FFT-based computation in single and double precision for small inputs.}
	\label{fig:fft_double_faster_compute_time}
\end{figure}

Another feature is illustrated in Figure \ref{fig:fft_double_faster_compute_time}, where we show computation time for inputs smaller than 128x128. From these measurements, we see that the cuFFT library computes inputs with matrix size up to 96x96 faster in double precision than in single precision, even though the double precision version works with twice the amount of data and uses operations with lower throughput. The cause is the \textit{Prepare} step, which allocates device memory and in this implementation runs the \textit{cufftPlan*} functions. Based on our measurements, the \textit{Prepare} step is slower for single precision computation than for double precision. Due to the closed source of the cuFFT library, we can only speculate on the cause, but it is most likely due to some additional precomputation done for the single version. 


\begin{figure}[ht]
	\centering	
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/fft_speedup_one_to_one_gpulab.pdf_tex}
		\caption{One to one.}
		\label{fig:fft_speedup_one_to_one}
	\end{subfigure}
	%\hfill
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/fft_speedup_one_to_many_gpulab.pdf_tex}
		\caption{One to many.}
		\label{fig:fft_speedup_one_to_many}
	\end{subfigure}
	%\hfill
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/fft_speedup_n_to_mn_gpulab.pdf_tex}
		\caption{N to MN.}
		\label{fig:fft_speedup_n_to_mn}
	\end{subfigure}
	%\hfill
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/fft_speedup_n_to_m_gpulab.pdf_tex}
		\caption{N to M.}
		\label{fig:fft_speedup_n_to_m}
	\end{subfigure}
	\caption{Speedup of the Basic definition-based implementation and best definition-based optimized implementation compared to FFT-based implementation on gpulab.}
	\label{fig:fft_speedup}
\end{figure}

When comparing the FFT-based implementation with the definition-based implementations, we are mostly interested in the input matrix size where the definition-based and FFT-based implementations are equal and how that size differs between the Basic algorithm and the best of our optimizations, which we now collectively call the \textit{Optimized} algorithm. We call such matrix size the \textit{equality point}, as it is represented by a point where the graphs of the implementations intersect in the diagram. The \textit{Optimized} implementation is chosen for each input (i.e., a combination of input matrix size and number of input matrices) separately. In this section, we measure the whole computation, including allocation, transfer to the GPU, kernel execution, transfer from the GPU, and deallocation. This is because the FFT-based algorithm differs from the definition-based algorithm in all of these stages. Most importantly, we need to include the overhead of the Prepare step that has a major impact on the total run time as described in the previous paragraph.

First, we measure with the same matrix sizes as in previous sections, as shown in Figure \ref{fig:fft_speedup}. The figure shows the speedup of the Basic and Optimized definition-based algorithm compared to the FFT-based implementation, which is used as a baseline. As described above, cuFFT library used by the FFT-based implementation is optimized for certain input sizes, with powers of two being the fastest. As such, measurements in Figure \ref{fig:fft_speedup} are optimal for the FFT-based implementation. The figure shows an improvement in the position of the equality point for all input types.



The most noticeable change between Basic and Optimized implementation is in the \textit{one-to-one} type, shown in Figure \ref{fig:fft_speedup_one_to_one}. Due to the small input data size, the GPU cannot be fully saturated by the FFT-based implementation. Even though this change is the most noticeable, it is the least important, as the total execution time is very low for all implementations of this input type. 

For the \textit{one-to-many} type in Figure \ref{fig:fft_speedup_one_to_many}, the results again show great improvement for small input data sizes, here represented by small number of right input matrices. For these sizes, the Basic implementation often did not reach the speed of the FFT-based algorithm for any measured input matrix size, whereas the optimized implementation has its equality point above the matrix size of 100x100. For inputs with a large number of right input matrices, we see that the equality point moves to around 64x64 input matrices.

The \textit{n-to-mn} type in Figure \ref{fig:fft_speedup_n_to_mn} again displays great similarity to the \textit{one-to-many} type due to the way it is implemented. The equality points are at similar positions, being slightly below 64x64 for the largest input.

The \textit{n-to-m} type in Figure \ref{fig:fft_speedup_n_to_m} shows surprisingly good performance even for the Basic implementation. For inputs with a small number of matrices, optimized equality points are all above 120x120 matrix size. For inputs with a large number of matrices, the equality point is just below 80x80.


When we compare the behavior between gpulab and laptop, as shown in Figure \ref{fig:fft_speedup_laptop}, we see that the FFT-based implementation suffers massively with low occupancy. For smaller input sizes, we see over 100 times improvement for both the \textit{one-to-one} type and for small number of matrices in the \textit{n-to-mn} type. Once the input size becomes large enough, as shown by the 100 to 1000 input for the \textit{n-to-mn} type, the behavior is very similar to that of gpulab, but with much bigger improvements for smaller input matrix sizes.


\begin{figure}[ht]
	\centering	
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/fft_speedup_one_to_one.pdf_tex}
		\caption{One to one.}
		%\label{fig:fft_speedup_one_to_one}
	\end{subfigure}
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/fft_speedup_n_to_mn.pdf_tex}
		\caption{N to MN.}
		%\label{fig:fft_speedup_n_to_mn}
	\end{subfigure}
	\caption{Speedup of the Basic definition-based implementation and best definition-based optimized implementation compared to FFT-based implementation on laptop.}
	\label{fig:fft_speedup_laptop}
\end{figure}


\begin{figure}[ht]
	\centering	
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/fft_speedup_one_to_one_anti_fft_gpulab.pdf_tex}
		\caption{One to one.}
		\label{fig:fft_speedup_antifft_one_to_one}
	\end{subfigure}
	%\hfill
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/fft_speedup_one_to_many_anti_fft_gpulab.pdf_tex}
		\caption{One to many.}
		\label{fig:fft_speedup_antifft_one_to_many}
	\end{subfigure}
	%\hfill
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/fft_speedup_n_to_mn_anti_fft_gpulab.pdf_tex}
		\caption{N to MN.}
		\label{fig:fft_speedup_antifft_n_to_mn}
	\end{subfigure}
	%\hfill
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/fft_speedup_n_to_m_anti_fft_gpulab.pdf_tex}
		\caption{N to M.}
		\label{fig:fft_speedup_antifft_n_to_m}
	\end{subfigure}
	\caption{Speedup when measured in matrix sizes which are prime numbers.}
	\label{fig:fft_speedup_antifft}
\end{figure}

The second set of results, shown in Figure \ref{fig:fft_speedup_antifft}, changes the matrix sizes for which we measure the implementations. We take the matrix sizes used to measure the original results in Figure \ref{fig:fft_speedup} and find the closest prime number. For example 64x64 becomes 61x61, 128x128 becomes 127x127 and 256x256 becomes 257x257. This should make the FFT-based implementation slower, making the improvements provided by our optimizations of definition-based implementation greater as the definition-based implementation is not that closely dependent on the precise size of the input matrices.

We see a major change in the behavior of \textit{one-to-one} type, shown in Figure \ref{fig:fft_speedup_antifft_one_to_one}. The equality point moves beyond the 256x256 matrix size, and we see two peaks in the improvement as the FFT-based implementation becomes faster for the 127x127 input before slowing down again for the 157x157 matrix size. 

% TODO: Expand this
For other input types, we also see an improvement compared to the previous measurements, even if not as significant as for the \textit{one-to-one} type. The speedup values are better across the board, with equality points moving to larger matrix sizes. A similar change is visible when comparing the two measurements on laptop.

\begin{figure}[ht]
	\centering	

	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/fft_startup_one_to_many_gpulab.pdf_tex}
		\caption{One to many.}
		\label{fig:fft_startup_one_to_many}
	\end{subfigure}
	%\hfill
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/fft_startup_n_to_mn_gpulab.pdf_tex}
		\caption{N to MN.}
		\label{fig:fft_startup_n_to_mn}
	\end{subfigure}
	%\hfill
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/fft_startup_n_to_m_gpulab.pdf_tex}
		\caption{N to M.}
		\label{fig:fft_startup_n_to_m}
	\end{subfigure}
	\caption{Speedup of the Basic definition-based implementation and best definition-based optimized implementation compared to FFT-based implementation.}
	\label{fig:fft_startup}
\end{figure}

Due to the many iterations of the algorithm which are measured together, the measured time reflects many repeated computations on the same input data. This results in caches being fully populated and allows the cuFFT library possible internal caching. Figure \ref{fig:fft_startup} shows a measurement of a single iteration which limits or possibly even removes the ability for internal caching in the cuFFT library and populating of any caches, measuring \textit{warm-up performance} instead of \textit{sustainable performance}. This type of measurement better reflects the usual usage of a cross-correlation computing function, as it is not useful to compute cross-correlation for the same input multiple times.

This measurement can only be done for large input sizes, as the background noise when measuring smaller input sizes would make any measured results useless. In Figures \ref{fig:fft_startup_one_to_many} and \ref{fig:fft_startup_n_to_mn} we see that the equality point is around 90x90, which is an improvement compared to previous measurements. This hints at some internal caching done by the cuFFT library between computations. The equality point in Figure \ref{fig:fft_startup_n_to_m} is also improved, now just below 60x60.

\subsection{SciPy}
% Reference Scipy implementation

\begin{figure}[ht]
	\centering	
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/scipy_one_to_one_gpulab.pdf_tex}
		\caption{One to one.}
	\end{subfigure}
	%\hfill
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/scipy_one_to_many_gpulab.pdf_tex}
		\caption{One to many.}
	\end{subfigure}
	%\hfill
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/scipy_n_to_mn_gpulab.pdf_tex}
		\caption{N to MN.}
	\end{subfigure}
	%\hfill
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/scipy_n_to_m_gpulab.pdf_tex}
		\caption{N to M.}
		\label{fig:scipy_n_to_m_gpulab}
	\end{subfigure}
	\caption{Speedup of the best definition-based implementation in this thesis compared to SciPy on gpulab.}
	\label{fig:scipy_speedup_gpulab}
\end{figure}


We compare the best of the implementations provided by this thesis with Scipy in Figure \ref{fig:scipy_speedup_gpulab}. Even though SciPy uses multithreaded FFT-based implementation, for any input with an input matrix size above 8x8, the definition-based CUDA C++ implementation is many times faster. We see the speedup increasing with increasing input size for all input types, showcasing the much higher throughput of the GPU. In terms of speedup, we see the \textit{one-to-one} achieving the lowest speedup due to limited occupancy due to small input size, with all other input types achieving up to $3000$ times speedup for the largest input sizes. In terms of absolute times, which range from $0.13$~s for the \textit{one-to-one} type to $2.03$~s for the \textit{n-to-m} type for SciPy, we see improvement to $0.32$~ms and $0.66$~ms respectively.

One figure that stands out is Figure \ref{fig:scipy_n_to_m_gpulab}, where the maximum speedup is not achieved for the largest input in terms of the number of matrices but for a smaller with 4 to 4 matrices. We also see that the speedup does not increase linearly with increased input matrix size apart from the input with 4 to 4 input matrices. This is caused by the algorithm with \textit{multimat\_both} and work distribution optimizations being the fastest for the small input sizes. Once the GPU is sufficiently saturated, another algorithm, this time with both \textit{multimat\_both} and \textit{multirow\_both} optimizations, achieves the highest speedup, which increases at a different rate than for the first algorithm.

For comparison, we show the \textit{n-to-mn} and \textit{n-to-m} input types measured on laptop in Figure \ref{fig:scipy_speedup}. As expected, the behavior is generally the same as on gpulab, only with a smaller total speedup due to the lower performance of the laptop GPU.

In summary, the results are as expected, with SciPy being faster for the smallest inputs as the latency of data transfer and kernel to and from the GPU and the overhead of kernel start are independent of the data size and will result in a slower total speed of the GPU implementation. For all larger inputs, the generally slow speed of Python and limited parallelism of the CPU results in the GPU CUDA C++ implementation being faster.


\begin{figure}[ht]
	\centering	

	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/scipy_n_to_mn.pdf_tex}
		\caption{N to MN.}
	\end{subfigure}
	%\hfill
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/scipy_n_to_m.pdf_tex}
		\caption{N to M.}
	\end{subfigure}
	\caption{Speedup of the best definition-based implementation in this thesis compared to SciPy on laptop.}
	\label{fig:scipy_speedup}
\end{figure}

\subsection{Matlab}
% Reference matlab implementation https://www.mathworks.com/help/signal/ref/xcorr2.html?s_tid=mwa_osa_a

\begin{figure}[ht]
	\centering	
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/matlab_one_to_one.pdf_tex}
		\caption{One to one.}
		\label{fig:matlab_one_to_one}
	\end{subfigure}
	%\hfill
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/matlab_one_to_many.pdf_tex}
		\caption{One to many.}
	\end{subfigure}
	%\hfill
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/matlab_n_to_mn.pdf_tex}
		\caption{N to MN.}
	\end{subfigure}
	%\hfill
	\begin{subfigure}{0.4\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\input{./img/matlab_n_to_m.pdf_tex}
		\caption{N to M.}

	\end{subfigure}
	\caption{Speedup of the best definition-based implementation in this thesis compared to Matlab on laptop.}
	\label{fig:matlab_speedup}
\end{figure}

In this section, we compare the best of the implementations provided by this thesis with Matlab. The results of the comparison are shown in Figure \ref{fig:matlab_speedup}. As the Matlab implementation of 2D cross-correlation we measure against utilizes a GPU, the total speedup compared to the speedup seen in the previous section is much lower. As Matlab only implements the \textit{one-to-one} type of cross-correlation, the best comparison between the implementations is in Figure \ref{fig:matlab_one_to_one}, where we see that our implementation gets progressively faster. As we expect Matlab to utilize FFT-based implementation, the speedup should decrease for larger input sizes. Unfortunately, due to time constraints, we were unable to measure where the equality point is.

For other input types, we sequentially execute the \textit{one-to-one} input type for each pair of input matrices to be cross-correlated. As expected, this makes Matlab much slower for the smaller input matrix sizes, as it cannot saturate the GPU. For larger input sizes, the execution of each cross-correlation starts saturating the GPU, achieving almost parity with our implementation.