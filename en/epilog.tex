\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}
 
In this thesis, we have analyzed the definition-based cross-correlation algorithm, searching for parallelization and optimization possibilities based on the input matrix size and input type. Based on this analysis, we have implemented two algorithm families, each with many implementations employing different combinations of the optimization and parallelization strategies. The implementations are based on the CUDA framework.

The optimizations target many algorithm properties, from caching and data reuse to reduce the number of accesses to slower memory types, to the amount of parallelism available to fully saturate the GPU and utilize the full throughput provided by the hardware. Each optimization was rigorously profiled and measured for different input sizes and input types, some trying to optimize across all inputs and some targeting specific combinations of input sizes and types. 

The first algorithm family presented by this thesis was the Warp shuffle family, which utilizes warp shuffle instructions to share input data between threads of a warp, significantly reducing the number of accesses to global memory. The family includes further optimizations which reduce the total number of warp shuffle instructions required by computing cross-correlation between one matrix and many other matrices at once, or by computing multiple results from the same cross-correlation at once.  Another optimization employed by this family improves parallelization and load balancing. 
 
The second algorithm family was the Warp per shift family, which improves occupancy by utilizing whole warp for computation of a single element in the output matrix. This family includes optimizations for data reuse and cooperation of warps through shared memory, together with further improvements to occupancy by utilizing multiple warps per single output matrix element.

We then compared these implementations against several implementations, first with a Basic definition-based CUDA  implementation, second based on the Fast Fourier Transform algorithm also using CUDA, third the CPU based implementation provided by the Python SciPy library and fourth a GPU based implementation provided by Matlab. 

When compared with the Basic definition-based CUDA implementation, our optimized implementation achieves at least 5 times speedup for most inputs and up to 10 to 80 times speedup for certain input sizes. When compared with the FFT-based implementation, we achieve speed parity for input matrix sizes over 100x100 for smaller number of input matrices, decreasing down to 60x60 for larger number of matrices. In comparison with the CPU based SciPy library, our implementations achieve 50 times speedup for small input sizes and up to 3000 times speedup for larger input sizes. When compared with Matlab, we achieve around 5 times speedup for small input sizes.
 


\section{Future work}
Although the optimized implementations provided in this thesis are useful and achieve the speedup listed above, they are mostly designed for easy instrumentation and benchmarking. This results in several restrictions on the size and form of the input data, long build times and giant executable. In the future, the implementation could be streamlined and the limitations removed.

This thesis also focused heavily on utilizing the CUDA platform for implementation of the optimized definition-based algorithms. Future work could be aimed at implementation using other, currently less common tools such as OpenCL. 

Next, the implementation in this thesis utilize only a single GPU. In the future, the implementation could be expanded to utilize more GPUs on a single system, or even further to utilize GPUs over multiple systems. Based on the inherent parallelism in the definition-based cross-correlation described in this thesis, the  implementation of distributed version could build on the work provided in this thesis very easily.

To further speed up the definition-based cross-correlation, additional properties of the input data could be used. This includes utilizing non-negative input data and terminating parts of the computation which cannot reach current maximum, or computing a submatrix of the full cross-correlation matrix.